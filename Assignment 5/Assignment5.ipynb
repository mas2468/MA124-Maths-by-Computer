{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA124 Maths by Computer: Assignment 5\n",
    "\n",
    "## Linear Algebra\n",
    "\n",
    "This assignment consists of completing the computational tasks specified in this notebook. Your plots should be clearly labelled. There is no additional write up. You do not need an introduction or conclusion or discussion of your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part A: Eigenvalues (10 Marks)\n",
    "\n",
    "In this part of the assignment you will compute and plot eigenvalues in the complex plane (eigenvalue spectra) for some typical matrices that arise in the study of partial differential equations. (The connection to partial differential equations will be covered in later modules.) \n",
    "\n",
    "1. Set `N=100`. Create a matrix `M1` consisting of all `-1` on the first super-diagonal and all `+1` on the first sub-diagonal.  Then put a `+1` in the top right corner and a `-1` in the bottom left corner. For `N=7` this matrix would look like this:\n",
    "\n",
    "$$\n",
    "M_1 = \n",
    "\\begin{pmatrix}\n",
    "0 & -1 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "1 & 0 & -1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & -1 & 0 & 0 & 0  \\\\\n",
    "0 & 0 & 1 & 0 & -1 & 0 & 0  \\\\\n",
    "0 & 0 & 0 & 1 & 0 & -1 & 0  \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0  & -1 \\\\\n",
    "-1 & 0 & 0 & 0 & 0 & 1 & 0 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "(You might want to first use `N=7` and print `M1`. Once you are sure that it is correct, you can remove the print statement and set `N=100`. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2. Form the matrix `M2` given by $M_2 = -\\frac{1}{4} M_1^4$, where $M_1^4$ is matrix product of $M_1$ with itself 4 times. (You can compute this using any of the methods for taking matrix products, but you cannot use `for loops`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3. Compute the eigenvalues of `M1` and `M2` and plot these in the complex plane. Produce a single plot with the eigenvalues for both matrices. Use a legend to indicate which eigenvalues correspond to which matrix. For this plot and all subsequent eigenvalue plot, use `plt.axis(\"square\")` to produce a square plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of eigenvalues is known as an eigenvalue spectrum. What you will learn later is that the imaginary part of an eigenvalue corresponds to an oscillation frequency while the real part corresponds to exponential decay (real part negative) or growth (real part positive). Hence, we see that $M_1$ corresponds to oscillatory behaviour while $M_2$ corresponds to exponential decay.\n",
    "\n",
    "---\n",
    "\n",
    "4. Form a matrix `M3` given by $M_3 = M_1 + M_2$. Compute and plot its eigenvalues in the complex plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the eigenvalue spectrum of $M_3$ shows a mixture of oscillation and decay. \n",
    "\n",
    "---\n",
    "\n",
    "5. Form a new matrix $A_{plus} = I + \\frac{1}{2} M_3$, where `I` is the $N \\times N$ identity matrix. Form another new matrix $A_{minus} = (I - \\frac{1}{2} M_3)^{-1}$, where $~^{-1}$ means matrix inverse. Matrices like these arise in the numerical solution of partial differential equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "6. Compute the eigenvalues of $A_{plus}$ and $A_{minus}$ and plot these in the complex plane. Produce a single plot with the eigenvalues for both matrices. Use a legend to indicate which eigenvalues correspond to which matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "7. Compute the eigenvalues of $A_{plus}^4$ and $A_{minus}^4$ and plot these in the complex plane. Produce a single plot with the eigenvalues for both matrices. You want to use `np.linalg.matrix_power` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should only submit your assignment for $A_{plus}^4$ and $A_{minus}^4$, but please look at other powers, e.g. 2, 3.\n",
    "\n",
    "---\n",
    "\n",
    "### Part B: Moore-Penrose Pseudo-Inverse (10 Marks) \n",
    "\n",
    "Let $T: \\mathbb R[x]_{\\le 5} \\to \\mathbb R[x]_{\\le 5}$ be given by $T(f) = df/dx$ in the basis of monomials: $1, x, x^2, x^3, x^4, x^5$.\n",
    "\n",
    "1. Write Python code to generate the $6 \\times 6$ matrix $D$ for this linear transformation. Print the matrix $D$. \n",
    "\n",
    "    (For full marks, you need to do the following. Set `K=6`. Generate a vector `d` with values 1 through K-1. Use `diag` to generate the matrix `D` from the vector `d`, with `d` on the first super-diagonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let the vector $v$ be the coordinates of $f$ in the monomial basis, e.g. $f=1+x$ gives $v = (1,1,0,0,0,0)$. You can differentiate this polynomial by multiplying by the matrix $D$. \n",
    "\n",
    "2. Let $f = 2 - 3x - 4x^2 + 5 x^3 + 6x^4 - 7x^5$. Form the coordinate vector $v$. Compute $w = Dv$ and print the resulting vector $w$. (You should be happy that the resulting vector is the coordinate vector of $df/dx$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3. Compute and print the determinant of $D$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D$ is singular and hence $D$ does not have an inverse. \n",
    "\n",
    "$D$ corresponds to taking a derivative in the space of monomials of degree $\\le 5$. Inverting $D$ corresponds to integration. We know that we cannot invert $D$ because integration take us out of this space, e.g. the integral of $x^5$ is not in our space. Related to this is that integration is not unique; there is an undetermined constant of integration. Equivalently, $D$ has a non-trivial kernel. You should know what the kernel of $D$ is.\n",
    "\n",
    "However, $D$ has a pseudo-inverse. \n",
    "\n",
    "4. Compute $D_{pinv}$, the pseudo-inverse of $D$. Compute and print $v_2 = D_{pinv} w$. (You should be satisfied that multiplying by $D_{pinv}$ does in fact correspond to integration with a particular choice of the constant of integration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Print $D_{pinv}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part C: Image Compression and Edge Detection (5 Marks)\n",
    "\n",
    "Most of this part of the assignment has already been completed for you. A greyscale image of $M \\times N$ pixels is effectively a $M \\times N$ matrix with each matrix element corresponding to the greyscale value at the corresponding pixel. Here we exploit this to have some fun.\n",
    "\n",
    "The cell below imports some modules from skimage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import img_as_float\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We load an image (here an espresso cup), convert the image to floats and convert the colour to greyscale. The result is a $400 \\times 600$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = rgb2gray(img_as_float(data.coffee()))\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using matplotlib it is possible to plot the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following function demonstrates how the singular value decomposition (SVD) could be used to compress an image. The function has as input a matrix (corresponding to an image). The function computes the SVD of the matrix. From the SVD the function generates a new matrix (image) using only half of the singular values. \n",
    "\n",
    "(This function does not actually save the new image in a format that saves disk space, but it could. The function is meant to demonstrate how it is possible to extract the information content from the image using the SVD and reconstruct a nearly equivalent image using much less information .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_svd(matrix):\n",
    "    U, s, VT = np.linalg.svd(matrix, full_matrices=False)\n",
    "    orig_size = np.size(matrix)\n",
    "    N = int(0.5 * len(s))\n",
    "    U = U[:,:N]\n",
    "    s = s[:N]\n",
    "    VT = VT[:N,:]\n",
    "    new_size = np.size(U) + np.size(s) + np.size(VT)\n",
    "    print(\"compression ratio =\", int(100*(new_size/orig_size)), \"%\")\n",
    "    return U @ np.diag(s) @ VT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here we call the demonstration function and then plot both the original image and the new image after the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_compress = compress_svd(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix,cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(matrix_compress,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Edit the above function to use only 25% of the singular values. Then edit the above function to use only 10% of the singular values. (For the assignment submission, use 10%. You are not mistaken. What you are being asked to do here is extremely simple.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "At the beginning of this assignment you created a $100 \\times 100$ matrix $M_1$. \n",
    "\n",
    "2. Repeat that code in the cell below but with $N=400$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new image matrix `edge = M1 @ matrix` and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how linear algebra can be used for edge detection in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "You should submit **this Jupyter notebook only**. **This must be a `.ipynb` file, not a pdf file or any other file type**. You need to clearly label your plots. However, you do not need to write an introduction! You do not need to discuss your results! There are no additional marks based on overall quality and clarity of the submission.  Simply complete all the computational tasks in this notebook. \n",
    "\n",
    "The last thing you should do before submitting the notebook is to Restart Kernel and Run All Cells. You should then save the notebook and submit the .ipynb file. **You will lose one mark if you submit a notebook that has not been run.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
